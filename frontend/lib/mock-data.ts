// frontend/lib/mock-data.ts
import { Topic } from "./types";

export const mockTopics: Topic[] = [
    {
        id: "1",
        title: "Reinforced RAG: Adaptive Retrieval Policies for Long-Context LLMs",
        summary:
            "A new approach that uses reinforcement learning to optimize how LLMs retrieve chunks from large vector stores, reducing hallucinations and latency.",
        source: "arXiv",
        scores: {
            trendiness: 9.32,
            technicalDepth: 8.20,
            practicality: 8.6,
        },
        tags: ["RAG", "LLMs", "Retrieval", "Optimization"],
    },
    {
        id: "2",
        title: "Lightweight Agentic Framework for Production-Scale Workflows",
        summary:
            "A practical agent framework focusing on tool orchestration, task graphs, and observability for real-world backend systems.",
        source: "Blog",
        scores: {
            trendiness: 7,
            technicalDepth: 7,
            practicality: 9,
        },
        tags: ["Agents", "Production", "Orchestration"],
    },
    {
        id: "3",
        title: "KV-Cache Compression Techniques for Serving 100K+ Token Contexts",
        summary:
            "Survey and benchmarks of different KV-cache compression and eviction strategies for cost-efficient long-context inference.",
        source: "Substack",
        scores: {
            trendiness: 8,
            technicalDepth: 9,
            practicality: 7,
        },
        tags: ["Inference", "KV-Cache", "Long Context"],
    },
    {
        id: "4",
        title: "Lightweight Agentic Framework for Production-Scale Workflows",
        summary:
            "A practical agent framework focusing on tool orchestration, task graphs, and observability for real-world backend systems.",
        source: "Blog",
        scores: {
            trendiness: 7,
            technicalDepth: 7,
            practicality: 9,
        },
        tags: ["Agents", "Production", "Orchestration"],
    },
    {
        id: "5",
        title: "KV-Cache Compression Techniques for Serving 100K+ Token Contexts",
        summary:
            "Survey and benchmarks of different KV-cache compression and eviction strategies for cost-efficient long-context inference.",
        source: "Substack",
        scores: {
            trendiness: 8,
            technicalDepth: 9,
            practicality: 7,
        },
        tags: ["Inference", "KV-Cache", "Long Context"],
    },
    {
        id: "6",
        title: "Lightweight Agentic Framework for Production-Scale Workflows",
        summary:
            "A practical agent framework focusing on tool orchestration, task graphs, and observability for real-world backend systems.",
        source: "Blog",
        scores: {
            trendiness: 7,
            technicalDepth: 7,
            practicality: 9,
        },
        tags: ["Agents", "Production", "Orchestration"],
    },
    {
        id: "7",
        title: "KV-Cache Compression Techniques for Serving 100K+ Token Contexts",
        summary:
            "Survey and benchmarks of different KV-cache compression and eviction strategies for cost-efficient long-context inference.",
        source: "Substack",
        scores: {
            trendiness: 8,
            technicalDepth: 9,
            practicality: 7,
        },
        tags: ["Inference", "KV-Cache", "Long Context"],
    },
    {
        id: "8",
        title: "Lightweight Agentic Framework for Production-Scale Workflows",
        summary:
            "A practical agent framework focusing on tool orchestration, task graphs, and observability for real-world backend systems.",
        source: "Blog",
        scores: {
            trendiness: 7,
            technicalDepth: 7,
            practicality: 9,
        },
        tags: ["Agents", "Production", "Orchestration"],
    },
    {
        id: "9",
        title: "KV-Cache Compression Techniques for Serving 100K+ Token Contexts",
        summary:
            "Survey and benchmarks of different KV-cache compression and eviction strategies for cost-efficient long-context inference.",
        source: "Substack",
        scores: {
            trendiness: 8,
            technicalDepth: 9,
            practicality: 7,
        },
        tags: ["Inference", "KV-Cache", "Long Context"],
    }
];


export const oldChats = [
    {id: "1", title: "KV-Cache Compression Notes"},
    {id: "2", title: "RAG Retrieval Tuning"},
    {id: "3", title: "Agent Workflow Design"},
    {id: "4", title: "KV-Cache Compression Notes"},
    {id: "5", title: "RAG Retrieval Tuning"},
    {id: "6", title: "Agent Workflow Design"},
    {id: "7", title: "KV-Cache Compression Notes"},
    {id: "8", title: "RAG Retrieval Tuning"},
    {id: "9", title: "Agent Workflow Design"},
    {id: "10", title: "KV-Cache Compression Notes"},
    {id: "11", title: "RAG Retrieval Tuning"},
    {id: "12", title: "The Agent Workflow Design"},
    {id: "13", title: "KV-Cache Compression Notes"},
    {id: "14", title: "RAG Retrieval Tuning"},
    {id: "15", title: "15 The Agent Workflow Design"},
];